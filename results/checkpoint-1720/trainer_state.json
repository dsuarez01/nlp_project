{
  "best_metric": 1.0109559297561646,
  "best_model_checkpoint": "./results/checkpoint-1720",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1720,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005813953488372093,
      "grad_norm": 5.344817638397217,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.6628,
      "step": 10
    },
    {
      "epoch": 0.011627906976744186,
      "grad_norm": 4.839662075042725,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.6694,
      "step": 20
    },
    {
      "epoch": 0.01744186046511628,
      "grad_norm": 2.981722354888916,
      "learning_rate": 3e-06,
      "loss": 1.666,
      "step": 30
    },
    {
      "epoch": 0.023255813953488372,
      "grad_norm": 4.067765235900879,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6223,
      "step": 40
    },
    {
      "epoch": 0.029069767441860465,
      "grad_norm": 3.809563636779785,
      "learning_rate": 5e-06,
      "loss": 1.6455,
      "step": 50
    },
    {
      "epoch": 0.03488372093023256,
      "grad_norm": 4.355715274810791,
      "learning_rate": 6e-06,
      "loss": 1.6543,
      "step": 60
    },
    {
      "epoch": 0.040697674418604654,
      "grad_norm": 4.519154071807861,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.6471,
      "step": 70
    },
    {
      "epoch": 0.046511627906976744,
      "grad_norm": 4.081350326538086,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.6727,
      "step": 80
    },
    {
      "epoch": 0.05232558139534884,
      "grad_norm": 5.114100933074951,
      "learning_rate": 9e-06,
      "loss": 1.6142,
      "step": 90
    },
    {
      "epoch": 0.05813953488372093,
      "grad_norm": 6.365994930267334,
      "learning_rate": 1e-05,
      "loss": 1.6457,
      "step": 100
    },
    {
      "epoch": 0.06395348837209303,
      "grad_norm": 3.7575159072875977,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.6433,
      "step": 110
    },
    {
      "epoch": 0.06976744186046512,
      "grad_norm": 3.6109530925750732,
      "learning_rate": 1.2e-05,
      "loss": 1.6478,
      "step": 120
    },
    {
      "epoch": 0.0755813953488372,
      "grad_norm": 3.876314401626587,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.646,
      "step": 130
    },
    {
      "epoch": 0.08139534883720931,
      "grad_norm": 4.670299530029297,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.6307,
      "step": 140
    },
    {
      "epoch": 0.0872093023255814,
      "grad_norm": 4.7191972732543945,
      "learning_rate": 1.5e-05,
      "loss": 1.638,
      "step": 150
    },
    {
      "epoch": 0.09302325581395349,
      "grad_norm": 4.216283321380615,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.6352,
      "step": 160
    },
    {
      "epoch": 0.09883720930232558,
      "grad_norm": 2.761888027191162,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.6265,
      "step": 170
    },
    {
      "epoch": 0.10465116279069768,
      "grad_norm": 2.662081718444824,
      "learning_rate": 1.8e-05,
      "loss": 1.6027,
      "step": 180
    },
    {
      "epoch": 0.11046511627906977,
      "grad_norm": 6.468395709991455,
      "learning_rate": 1.9e-05,
      "loss": 1.6179,
      "step": 190
    },
    {
      "epoch": 0.11627906976744186,
      "grad_norm": 4.636982440948486,
      "learning_rate": 2e-05,
      "loss": 1.6034,
      "step": 200
    },
    {
      "epoch": 0.12209302325581395,
      "grad_norm": 4.1472249031066895,
      "learning_rate": 2.1e-05,
      "loss": 1.6025,
      "step": 210
    },
    {
      "epoch": 0.12790697674418605,
      "grad_norm": 4.417694091796875,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.6122,
      "step": 220
    },
    {
      "epoch": 0.13372093023255813,
      "grad_norm": 3.8998019695281982,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.5921,
      "step": 230
    },
    {
      "epoch": 0.13953488372093023,
      "grad_norm": 2.780632734298706,
      "learning_rate": 2.4e-05,
      "loss": 1.6051,
      "step": 240
    },
    {
      "epoch": 0.14534883720930233,
      "grad_norm": 5.588603496551514,
      "learning_rate": 2.5e-05,
      "loss": 1.6071,
      "step": 250
    },
    {
      "epoch": 0.1511627906976744,
      "grad_norm": 4.233633995056152,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.5917,
      "step": 260
    },
    {
      "epoch": 0.1569767441860465,
      "grad_norm": 4.563027381896973,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.5878,
      "step": 270
    },
    {
      "epoch": 0.16279069767441862,
      "grad_norm": 4.317965984344482,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.5693,
      "step": 280
    },
    {
      "epoch": 0.1686046511627907,
      "grad_norm": 5.928818225860596,
      "learning_rate": 2.9e-05,
      "loss": 1.5609,
      "step": 290
    },
    {
      "epoch": 0.1744186046511628,
      "grad_norm": 4.890976905822754,
      "learning_rate": 3e-05,
      "loss": 1.5224,
      "step": 300
    },
    {
      "epoch": 0.18023255813953487,
      "grad_norm": 3.836915969848633,
      "learning_rate": 3.1e-05,
      "loss": 1.4991,
      "step": 310
    },
    {
      "epoch": 0.18604651162790697,
      "grad_norm": 5.8758864402771,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.4807,
      "step": 320
    },
    {
      "epoch": 0.19186046511627908,
      "grad_norm": 4.463301658630371,
      "learning_rate": 3.3e-05,
      "loss": 1.5034,
      "step": 330
    },
    {
      "epoch": 0.19767441860465115,
      "grad_norm": 4.181633949279785,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.4546,
      "step": 340
    },
    {
      "epoch": 0.20348837209302326,
      "grad_norm": 4.4173126220703125,
      "learning_rate": 3.5e-05,
      "loss": 1.3889,
      "step": 350
    },
    {
      "epoch": 0.20930232558139536,
      "grad_norm": 5.658443450927734,
      "learning_rate": 3.6e-05,
      "loss": 1.3883,
      "step": 360
    },
    {
      "epoch": 0.21511627906976744,
      "grad_norm": 6.435222148895264,
      "learning_rate": 3.7e-05,
      "loss": 1.4478,
      "step": 370
    },
    {
      "epoch": 0.22093023255813954,
      "grad_norm": 4.089199542999268,
      "learning_rate": 3.8e-05,
      "loss": 1.3776,
      "step": 380
    },
    {
      "epoch": 0.22674418604651161,
      "grad_norm": 5.48319673538208,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.3594,
      "step": 390
    },
    {
      "epoch": 0.23255813953488372,
      "grad_norm": 5.369330883026123,
      "learning_rate": 4e-05,
      "loss": 1.3347,
      "step": 400
    },
    {
      "epoch": 0.23837209302325582,
      "grad_norm": 5.626660346984863,
      "learning_rate": 4.1e-05,
      "loss": 1.3083,
      "step": 410
    },
    {
      "epoch": 0.2441860465116279,
      "grad_norm": 5.621942520141602,
      "learning_rate": 4.2e-05,
      "loss": 1.2749,
      "step": 420
    },
    {
      "epoch": 0.25,
      "grad_norm": 6.8446364402771,
      "learning_rate": 4.3e-05,
      "loss": 1.2426,
      "step": 430
    },
    {
      "epoch": 0.2558139534883721,
      "grad_norm": 4.766554832458496,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.3383,
      "step": 440
    },
    {
      "epoch": 0.2616279069767442,
      "grad_norm": 4.072355270385742,
      "learning_rate": 4.5e-05,
      "loss": 1.2752,
      "step": 450
    },
    {
      "epoch": 0.26744186046511625,
      "grad_norm": 3.5227363109588623,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.4098,
      "step": 460
    },
    {
      "epoch": 0.27325581395348836,
      "grad_norm": 7.385791301727295,
      "learning_rate": 4.7e-05,
      "loss": 1.3232,
      "step": 470
    },
    {
      "epoch": 0.27906976744186046,
      "grad_norm": 3.2566094398498535,
      "learning_rate": 4.8e-05,
      "loss": 1.248,
      "step": 480
    },
    {
      "epoch": 0.28488372093023256,
      "grad_norm": 6.067791938781738,
      "learning_rate": 4.9e-05,
      "loss": 1.3242,
      "step": 490
    },
    {
      "epoch": 0.29069767441860467,
      "grad_norm": 5.277161121368408,
      "learning_rate": 5e-05,
      "loss": 1.2415,
      "step": 500
    },
    {
      "epoch": 0.29651162790697677,
      "grad_norm": 5.787474155426025,
      "learning_rate": 4.999708454810496e-05,
      "loss": 1.357,
      "step": 510
    },
    {
      "epoch": 0.3023255813953488,
      "grad_norm": 4.324771881103516,
      "learning_rate": 4.999416909620992e-05,
      "loss": 1.2765,
      "step": 520
    },
    {
      "epoch": 0.3081395348837209,
      "grad_norm": 3.63376522064209,
      "learning_rate": 4.999125364431487e-05,
      "loss": 1.2617,
      "step": 530
    },
    {
      "epoch": 0.313953488372093,
      "grad_norm": 3.7913615703582764,
      "learning_rate": 4.9988338192419825e-05,
      "loss": 1.3182,
      "step": 540
    },
    {
      "epoch": 0.31976744186046513,
      "grad_norm": 5.068334102630615,
      "learning_rate": 4.9985422740524786e-05,
      "loss": 1.3351,
      "step": 550
    },
    {
      "epoch": 0.32558139534883723,
      "grad_norm": 5.070766448974609,
      "learning_rate": 4.998250728862974e-05,
      "loss": 1.2623,
      "step": 560
    },
    {
      "epoch": 0.3313953488372093,
      "grad_norm": 5.812315940856934,
      "learning_rate": 4.9979591836734694e-05,
      "loss": 1.2594,
      "step": 570
    },
    {
      "epoch": 0.3372093023255814,
      "grad_norm": 5.572144985198975,
      "learning_rate": 4.9976676384839655e-05,
      "loss": 1.2571,
      "step": 580
    },
    {
      "epoch": 0.3430232558139535,
      "grad_norm": 6.6038713455200195,
      "learning_rate": 4.997376093294461e-05,
      "loss": 1.2318,
      "step": 590
    },
    {
      "epoch": 0.3488372093023256,
      "grad_norm": 4.4959611892700195,
      "learning_rate": 4.997084548104957e-05,
      "loss": 1.1849,
      "step": 600
    },
    {
      "epoch": 0.3546511627906977,
      "grad_norm": 4.174454212188721,
      "learning_rate": 4.996793002915452e-05,
      "loss": 1.1857,
      "step": 610
    },
    {
      "epoch": 0.36046511627906974,
      "grad_norm": 4.57239294052124,
      "learning_rate": 4.996501457725948e-05,
      "loss": 1.2917,
      "step": 620
    },
    {
      "epoch": 0.36627906976744184,
      "grad_norm": 7.956607341766357,
      "learning_rate": 4.996209912536444e-05,
      "loss": 1.1983,
      "step": 630
    },
    {
      "epoch": 0.37209302325581395,
      "grad_norm": 4.924173831939697,
      "learning_rate": 4.995918367346939e-05,
      "loss": 1.2302,
      "step": 640
    },
    {
      "epoch": 0.37790697674418605,
      "grad_norm": 5.075796127319336,
      "learning_rate": 4.9956268221574346e-05,
      "loss": 1.1873,
      "step": 650
    },
    {
      "epoch": 0.38372093023255816,
      "grad_norm": 5.35274600982666,
      "learning_rate": 4.99533527696793e-05,
      "loss": 1.1299,
      "step": 660
    },
    {
      "epoch": 0.38953488372093026,
      "grad_norm": 3.5825254917144775,
      "learning_rate": 4.995043731778426e-05,
      "loss": 1.2921,
      "step": 670
    },
    {
      "epoch": 0.3953488372093023,
      "grad_norm": 4.8447651863098145,
      "learning_rate": 4.9947521865889215e-05,
      "loss": 1.2667,
      "step": 680
    },
    {
      "epoch": 0.4011627906976744,
      "grad_norm": 4.21432638168335,
      "learning_rate": 4.994460641399417e-05,
      "loss": 1.3296,
      "step": 690
    },
    {
      "epoch": 0.4069767441860465,
      "grad_norm": 6.091492652893066,
      "learning_rate": 4.994169096209913e-05,
      "loss": 1.2017,
      "step": 700
    },
    {
      "epoch": 0.4127906976744186,
      "grad_norm": 4.121341228485107,
      "learning_rate": 4.9938775510204084e-05,
      "loss": 1.2496,
      "step": 710
    },
    {
      "epoch": 0.4186046511627907,
      "grad_norm": 4.587219715118408,
      "learning_rate": 4.9935860058309045e-05,
      "loss": 1.2679,
      "step": 720
    },
    {
      "epoch": 0.42441860465116277,
      "grad_norm": 6.595188140869141,
      "learning_rate": 4.993294460641399e-05,
      "loss": 1.2748,
      "step": 730
    },
    {
      "epoch": 0.43023255813953487,
      "grad_norm": 4.112225532531738,
      "learning_rate": 4.993002915451895e-05,
      "loss": 1.3197,
      "step": 740
    },
    {
      "epoch": 0.436046511627907,
      "grad_norm": 6.9001145362854,
      "learning_rate": 4.992711370262391e-05,
      "loss": 1.2306,
      "step": 750
    },
    {
      "epoch": 0.4418604651162791,
      "grad_norm": 5.022490501403809,
      "learning_rate": 4.992419825072887e-05,
      "loss": 1.177,
      "step": 760
    },
    {
      "epoch": 0.4476744186046512,
      "grad_norm": 4.594059944152832,
      "learning_rate": 4.992128279883382e-05,
      "loss": 1.1694,
      "step": 770
    },
    {
      "epoch": 0.45348837209302323,
      "grad_norm": 4.929139614105225,
      "learning_rate": 4.9918367346938776e-05,
      "loss": 1.2046,
      "step": 780
    },
    {
      "epoch": 0.45930232558139533,
      "grad_norm": 3.408472776412964,
      "learning_rate": 4.9915451895043736e-05,
      "loss": 1.344,
      "step": 790
    },
    {
      "epoch": 0.46511627906976744,
      "grad_norm": 3.4108994007110596,
      "learning_rate": 4.991253644314869e-05,
      "loss": 1.3718,
      "step": 800
    },
    {
      "epoch": 0.47093023255813954,
      "grad_norm": 5.139749050140381,
      "learning_rate": 4.9909620991253644e-05,
      "loss": 1.15,
      "step": 810
    },
    {
      "epoch": 0.47674418604651164,
      "grad_norm": 5.964498996734619,
      "learning_rate": 4.99067055393586e-05,
      "loss": 1.1528,
      "step": 820
    },
    {
      "epoch": 0.48255813953488375,
      "grad_norm": 2.560750961303711,
      "learning_rate": 4.990379008746356e-05,
      "loss": 1.1031,
      "step": 830
    },
    {
      "epoch": 0.4883720930232558,
      "grad_norm": 5.355456352233887,
      "learning_rate": 4.990087463556852e-05,
      "loss": 1.2174,
      "step": 840
    },
    {
      "epoch": 0.4941860465116279,
      "grad_norm": 5.174363136291504,
      "learning_rate": 4.9897959183673474e-05,
      "loss": 1.151,
      "step": 850
    },
    {
      "epoch": 0.5,
      "grad_norm": 3.8284125328063965,
      "learning_rate": 4.989504373177843e-05,
      "loss": 1.1256,
      "step": 860
    },
    {
      "epoch": 0.5058139534883721,
      "grad_norm": 4.985899448394775,
      "learning_rate": 4.989212827988338e-05,
      "loss": 1.2123,
      "step": 870
    },
    {
      "epoch": 0.5116279069767442,
      "grad_norm": 3.8609509468078613,
      "learning_rate": 4.988921282798834e-05,
      "loss": 1.2084,
      "step": 880
    },
    {
      "epoch": 0.5174418604651163,
      "grad_norm": 4.467101097106934,
      "learning_rate": 4.98862973760933e-05,
      "loss": 1.2021,
      "step": 890
    },
    {
      "epoch": 0.5232558139534884,
      "grad_norm": 4.29211950302124,
      "learning_rate": 4.988338192419825e-05,
      "loss": 1.2406,
      "step": 900
    },
    {
      "epoch": 0.5290697674418605,
      "grad_norm": 4.490988254547119,
      "learning_rate": 4.988046647230321e-05,
      "loss": 1.1713,
      "step": 910
    },
    {
      "epoch": 0.5348837209302325,
      "grad_norm": 3.534698486328125,
      "learning_rate": 4.9877551020408165e-05,
      "loss": 1.1636,
      "step": 920
    },
    {
      "epoch": 0.5406976744186046,
      "grad_norm": 5.428921699523926,
      "learning_rate": 4.987463556851312e-05,
      "loss": 1.2133,
      "step": 930
    },
    {
      "epoch": 0.5465116279069767,
      "grad_norm": 2.980210781097412,
      "learning_rate": 4.9871720116618073e-05,
      "loss": 1.175,
      "step": 940
    },
    {
      "epoch": 0.5523255813953488,
      "grad_norm": 7.352084159851074,
      "learning_rate": 4.9868804664723034e-05,
      "loss": 1.1781,
      "step": 950
    },
    {
      "epoch": 0.5581395348837209,
      "grad_norm": 4.654239654541016,
      "learning_rate": 4.9865889212827995e-05,
      "loss": 1.1794,
      "step": 960
    },
    {
      "epoch": 0.563953488372093,
      "grad_norm": 3.700822114944458,
      "learning_rate": 4.986297376093295e-05,
      "loss": 1.1513,
      "step": 970
    },
    {
      "epoch": 0.5697674418604651,
      "grad_norm": 5.181572914123535,
      "learning_rate": 4.98600583090379e-05,
      "loss": 1.1533,
      "step": 980
    },
    {
      "epoch": 0.5755813953488372,
      "grad_norm": 4.737339496612549,
      "learning_rate": 4.985714285714286e-05,
      "loss": 1.0381,
      "step": 990
    },
    {
      "epoch": 0.5813953488372093,
      "grad_norm": 4.907345294952393,
      "learning_rate": 4.985422740524782e-05,
      "loss": 1.1233,
      "step": 1000
    },
    {
      "epoch": 0.5872093023255814,
      "grad_norm": 4.130285263061523,
      "learning_rate": 4.985131195335277e-05,
      "loss": 1.2447,
      "step": 1010
    },
    {
      "epoch": 0.5930232558139535,
      "grad_norm": 5.69711971282959,
      "learning_rate": 4.9848396501457726e-05,
      "loss": 1.1647,
      "step": 1020
    },
    {
      "epoch": 0.5988372093023255,
      "grad_norm": 3.4918947219848633,
      "learning_rate": 4.9845481049562687e-05,
      "loss": 1.1488,
      "step": 1030
    },
    {
      "epoch": 0.6046511627906976,
      "grad_norm": 4.117507457733154,
      "learning_rate": 4.984256559766764e-05,
      "loss": 1.2181,
      "step": 1040
    },
    {
      "epoch": 0.6104651162790697,
      "grad_norm": 7.51192045211792,
      "learning_rate": 4.98396501457726e-05,
      "loss": 1.0847,
      "step": 1050
    },
    {
      "epoch": 0.6162790697674418,
      "grad_norm": 3.374605178833008,
      "learning_rate": 4.983673469387755e-05,
      "loss": 1.0337,
      "step": 1060
    },
    {
      "epoch": 0.622093023255814,
      "grad_norm": 6.586380481719971,
      "learning_rate": 4.983381924198251e-05,
      "loss": 1.0824,
      "step": 1070
    },
    {
      "epoch": 0.627906976744186,
      "grad_norm": 2.393927574157715,
      "learning_rate": 4.983090379008747e-05,
      "loss": 1.0698,
      "step": 1080
    },
    {
      "epoch": 0.6337209302325582,
      "grad_norm": 2.872751235961914,
      "learning_rate": 4.9827988338192424e-05,
      "loss": 1.1659,
      "step": 1090
    },
    {
      "epoch": 0.6395348837209303,
      "grad_norm": 7.199051380157471,
      "learning_rate": 4.982507288629738e-05,
      "loss": 1.0683,
      "step": 1100
    },
    {
      "epoch": 0.6453488372093024,
      "grad_norm": 3.2361698150634766,
      "learning_rate": 4.982215743440233e-05,
      "loss": 1.1342,
      "step": 1110
    },
    {
      "epoch": 0.6511627906976745,
      "grad_norm": 6.725221633911133,
      "learning_rate": 4.981924198250729e-05,
      "loss": 1.0465,
      "step": 1120
    },
    {
      "epoch": 0.6569767441860465,
      "grad_norm": 4.151967525482178,
      "learning_rate": 4.981632653061225e-05,
      "loss": 1.0824,
      "step": 1130
    },
    {
      "epoch": 0.6627906976744186,
      "grad_norm": 4.500724792480469,
      "learning_rate": 4.98134110787172e-05,
      "loss": 1.0944,
      "step": 1140
    },
    {
      "epoch": 0.6686046511627907,
      "grad_norm": 6.8437628746032715,
      "learning_rate": 4.981049562682216e-05,
      "loss": 1.0909,
      "step": 1150
    },
    {
      "epoch": 0.6744186046511628,
      "grad_norm": 6.355253219604492,
      "learning_rate": 4.9807580174927116e-05,
      "loss": 1.1393,
      "step": 1160
    },
    {
      "epoch": 0.6802325581395349,
      "grad_norm": 5.5681962966918945,
      "learning_rate": 4.9804664723032077e-05,
      "loss": 1.1188,
      "step": 1170
    },
    {
      "epoch": 0.686046511627907,
      "grad_norm": 6.237045764923096,
      "learning_rate": 4.9801749271137024e-05,
      "loss": 1.1664,
      "step": 1180
    },
    {
      "epoch": 0.6918604651162791,
      "grad_norm": 4.018311023712158,
      "learning_rate": 4.9798833819241985e-05,
      "loss": 1.2461,
      "step": 1190
    },
    {
      "epoch": 0.6976744186046512,
      "grad_norm": 2.9614484310150146,
      "learning_rate": 4.979591836734694e-05,
      "loss": 1.1256,
      "step": 1200
    },
    {
      "epoch": 0.7034883720930233,
      "grad_norm": 3.7303357124328613,
      "learning_rate": 4.97930029154519e-05,
      "loss": 1.177,
      "step": 1210
    },
    {
      "epoch": 0.7093023255813954,
      "grad_norm": 4.083719253540039,
      "learning_rate": 4.979008746355685e-05,
      "loss": 1.1411,
      "step": 1220
    },
    {
      "epoch": 0.7151162790697675,
      "grad_norm": 3.5120913982391357,
      "learning_rate": 4.978717201166181e-05,
      "loss": 1.0369,
      "step": 1230
    },
    {
      "epoch": 0.7209302325581395,
      "grad_norm": 4.2911810874938965,
      "learning_rate": 4.978425655976677e-05,
      "loss": 1.106,
      "step": 1240
    },
    {
      "epoch": 0.7267441860465116,
      "grad_norm": 6.54039192199707,
      "learning_rate": 4.978134110787172e-05,
      "loss": 1.1029,
      "step": 1250
    },
    {
      "epoch": 0.7325581395348837,
      "grad_norm": 6.561648845672607,
      "learning_rate": 4.9778425655976676e-05,
      "loss": 1.1011,
      "step": 1260
    },
    {
      "epoch": 0.7383720930232558,
      "grad_norm": 10.362866401672363,
      "learning_rate": 4.977551020408163e-05,
      "loss": 1.1537,
      "step": 1270
    },
    {
      "epoch": 0.7441860465116279,
      "grad_norm": 6.753833293914795,
      "learning_rate": 4.977259475218659e-05,
      "loss": 1.1694,
      "step": 1280
    },
    {
      "epoch": 0.75,
      "grad_norm": 5.9498491287231445,
      "learning_rate": 4.976967930029155e-05,
      "loss": 1.077,
      "step": 1290
    },
    {
      "epoch": 0.7558139534883721,
      "grad_norm": 5.414182186126709,
      "learning_rate": 4.9766763848396506e-05,
      "loss": 1.1252,
      "step": 1300
    },
    {
      "epoch": 0.7616279069767442,
      "grad_norm": 5.039965629577637,
      "learning_rate": 4.976384839650146e-05,
      "loss": 1.115,
      "step": 1310
    },
    {
      "epoch": 0.7674418604651163,
      "grad_norm": 4.756295680999756,
      "learning_rate": 4.9760932944606414e-05,
      "loss": 0.9342,
      "step": 1320
    },
    {
      "epoch": 0.7732558139534884,
      "grad_norm": 4.388603687286377,
      "learning_rate": 4.9758017492711375e-05,
      "loss": 1.113,
      "step": 1330
    },
    {
      "epoch": 0.7790697674418605,
      "grad_norm": 4.306211471557617,
      "learning_rate": 4.975510204081633e-05,
      "loss": 1.1258,
      "step": 1340
    },
    {
      "epoch": 0.7848837209302325,
      "grad_norm": 3.398261785507202,
      "learning_rate": 4.975218658892128e-05,
      "loss": 0.966,
      "step": 1350
    },
    {
      "epoch": 0.7906976744186046,
      "grad_norm": 5.516090393066406,
      "learning_rate": 4.974927113702624e-05,
      "loss": 1.113,
      "step": 1360
    },
    {
      "epoch": 0.7965116279069767,
      "grad_norm": 2.931213140487671,
      "learning_rate": 4.97463556851312e-05,
      "loss": 0.9656,
      "step": 1370
    },
    {
      "epoch": 0.8023255813953488,
      "grad_norm": 4.083937168121338,
      "learning_rate": 4.974344023323616e-05,
      "loss": 1.1056,
      "step": 1380
    },
    {
      "epoch": 0.8081395348837209,
      "grad_norm": 5.440476894378662,
      "learning_rate": 4.9740524781341105e-05,
      "loss": 1.1415,
      "step": 1390
    },
    {
      "epoch": 0.813953488372093,
      "grad_norm": 4.295438289642334,
      "learning_rate": 4.9737609329446066e-05,
      "loss": 1.0976,
      "step": 1400
    },
    {
      "epoch": 0.8197674418604651,
      "grad_norm": 5.722084045410156,
      "learning_rate": 4.973469387755103e-05,
      "loss": 1.1215,
      "step": 1410
    },
    {
      "epoch": 0.8255813953488372,
      "grad_norm": 4.688209533691406,
      "learning_rate": 4.973177842565598e-05,
      "loss": 1.0837,
      "step": 1420
    },
    {
      "epoch": 0.8313953488372093,
      "grad_norm": 7.56588077545166,
      "learning_rate": 4.9728862973760935e-05,
      "loss": 1.0664,
      "step": 1430
    },
    {
      "epoch": 0.8372093023255814,
      "grad_norm": 7.721175193786621,
      "learning_rate": 4.972594752186589e-05,
      "loss": 1.1055,
      "step": 1440
    },
    {
      "epoch": 0.8430232558139535,
      "grad_norm": 4.315739631652832,
      "learning_rate": 4.972303206997085e-05,
      "loss": 0.9501,
      "step": 1450
    },
    {
      "epoch": 0.8488372093023255,
      "grad_norm": 5.002842903137207,
      "learning_rate": 4.9720116618075804e-05,
      "loss": 1.1706,
      "step": 1460
    },
    {
      "epoch": 0.8546511627906976,
      "grad_norm": 4.414116382598877,
      "learning_rate": 4.971720116618076e-05,
      "loss": 1.1028,
      "step": 1470
    },
    {
      "epoch": 0.8604651162790697,
      "grad_norm": 4.129942417144775,
      "learning_rate": 4.971428571428572e-05,
      "loss": 1.1417,
      "step": 1480
    },
    {
      "epoch": 0.8662790697674418,
      "grad_norm": 5.676031589508057,
      "learning_rate": 4.971137026239067e-05,
      "loss": 1.0667,
      "step": 1490
    },
    {
      "epoch": 0.872093023255814,
      "grad_norm": 7.541815280914307,
      "learning_rate": 4.970845481049563e-05,
      "loss": 1.1195,
      "step": 1500
    },
    {
      "epoch": 0.877906976744186,
      "grad_norm": 4.64780330657959,
      "learning_rate": 4.970553935860058e-05,
      "loss": 1.0835,
      "step": 1510
    },
    {
      "epoch": 0.8837209302325582,
      "grad_norm": 3.453284740447998,
      "learning_rate": 4.970262390670554e-05,
      "loss": 1.077,
      "step": 1520
    },
    {
      "epoch": 0.8895348837209303,
      "grad_norm": 3.420335054397583,
      "learning_rate": 4.96997084548105e-05,
      "loss": 1.1035,
      "step": 1530
    },
    {
      "epoch": 0.8953488372093024,
      "grad_norm": 5.036203861236572,
      "learning_rate": 4.9696793002915456e-05,
      "loss": 1.0757,
      "step": 1540
    },
    {
      "epoch": 0.9011627906976745,
      "grad_norm": 4.97052526473999,
      "learning_rate": 4.969387755102041e-05,
      "loss": 1.1128,
      "step": 1550
    },
    {
      "epoch": 0.9069767441860465,
      "grad_norm": 4.7516632080078125,
      "learning_rate": 4.9690962099125364e-05,
      "loss": 0.9424,
      "step": 1560
    },
    {
      "epoch": 0.9127906976744186,
      "grad_norm": 8.544651985168457,
      "learning_rate": 4.9688046647230325e-05,
      "loss": 1.0274,
      "step": 1570
    },
    {
      "epoch": 0.9186046511627907,
      "grad_norm": 3.8014538288116455,
      "learning_rate": 4.968513119533528e-05,
      "loss": 1.1123,
      "step": 1580
    },
    {
      "epoch": 0.9244186046511628,
      "grad_norm": 7.1775689125061035,
      "learning_rate": 4.968221574344023e-05,
      "loss": 1.0411,
      "step": 1590
    },
    {
      "epoch": 0.9302325581395349,
      "grad_norm": 6.330191612243652,
      "learning_rate": 4.9679300291545194e-05,
      "loss": 1.1196,
      "step": 1600
    },
    {
      "epoch": 0.936046511627907,
      "grad_norm": 5.159268856048584,
      "learning_rate": 4.967638483965015e-05,
      "loss": 0.9576,
      "step": 1610
    },
    {
      "epoch": 0.9418604651162791,
      "grad_norm": 5.173038959503174,
      "learning_rate": 4.967346938775511e-05,
      "loss": 0.9336,
      "step": 1620
    },
    {
      "epoch": 0.9476744186046512,
      "grad_norm": 4.6034698486328125,
      "learning_rate": 4.967055393586006e-05,
      "loss": 1.0762,
      "step": 1630
    },
    {
      "epoch": 0.9534883720930233,
      "grad_norm": 5.617574214935303,
      "learning_rate": 4.9667638483965016e-05,
      "loss": 0.8792,
      "step": 1640
    },
    {
      "epoch": 0.9593023255813954,
      "grad_norm": 4.469963550567627,
      "learning_rate": 4.966472303206997e-05,
      "loss": 0.9146,
      "step": 1650
    },
    {
      "epoch": 0.9651162790697675,
      "grad_norm": 2.8761215209960938,
      "learning_rate": 4.966180758017493e-05,
      "loss": 1.0914,
      "step": 1660
    },
    {
      "epoch": 0.9709302325581395,
      "grad_norm": 8.60420036315918,
      "learning_rate": 4.9658892128279885e-05,
      "loss": 1.0677,
      "step": 1670
    },
    {
      "epoch": 0.9767441860465116,
      "grad_norm": 4.594794273376465,
      "learning_rate": 4.965597667638484e-05,
      "loss": 1.1162,
      "step": 1680
    },
    {
      "epoch": 0.9825581395348837,
      "grad_norm": 3.788830041885376,
      "learning_rate": 4.96530612244898e-05,
      "loss": 1.0013,
      "step": 1690
    },
    {
      "epoch": 0.9883720930232558,
      "grad_norm": 5.404738903045654,
      "learning_rate": 4.9650145772594754e-05,
      "loss": 1.0169,
      "step": 1700
    },
    {
      "epoch": 0.9941860465116279,
      "grad_norm": 7.349539279937744,
      "learning_rate": 4.964723032069971e-05,
      "loss": 1.0831,
      "step": 1710
    },
    {
      "epoch": 1.0,
      "grad_norm": 7.153757095336914,
      "learning_rate": 4.964431486880466e-05,
      "loss": 1.0894,
      "step": 1720
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.0109559297561646,
      "eval_runtime": 38.2726,
      "eval_samples_per_second": 179.763,
      "eval_steps_per_second": 11.235,
      "step": 1720
    }
  ],
  "logging_steps": 10,
  "max_steps": 172000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3645600456376320.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
